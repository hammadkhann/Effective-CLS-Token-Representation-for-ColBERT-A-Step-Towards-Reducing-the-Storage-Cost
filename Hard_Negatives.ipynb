{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hard-Negatives.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwhr8T9c76I2FtuQwd8xx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammadkhann/Effective-Dense-Retrieval/blob/main/Hard_Negatives.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tuVp9VXEV9mI",
        "outputId": "93de2361-f9a7-4608-c324-b318cbc0d047"
      },
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install beir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers>=0.10.3\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 30.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=4626ca6bd2215d38088e2df9408aba7df9e93f2346cc2a66de46610335bf4ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk45Qo00V6FY"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from beir.retrieval.train import TrainRetriever\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.autonotebook import tqdm\n",
        "import pathlib, os, gzip, json\n",
        "import logging\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "Wuxd0XLyVn4Z",
        "outputId": "9c41b4fd-3acd-4008-bc61-eb5d5872ef3d"
      },
      "source": [
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n",
        "\n",
        "#### Download msmarco.zip dataset and unzip the dataset\n",
        "dataset = \"msmarco\"\n",
        "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
        "out_dir = os.path.join(pathlib.Path(__file__).parent.absolute(), \"datasets\")\n",
        "data_path = util.download_and_unzip(url, out_dir)\n",
        "\n",
        "### Load BEIR MSMARCO training dataset, this will be used for query and corpus for reference.\n",
        "corpus, queries, _ = GenericDataLoader(data_path).load(split=\"train\")\n",
        "\n",
        "#################################\n",
        "#### Parameters for Training ####\n",
        "#################################\n",
        "\n",
        "train_batch_size = 75           # Increasing the train batch size improves the model performance, but requires more GPU memory (O(n))\n",
        "max_seq_length = 350            # Max length for passages. Increasing it, requires more GPU memory (O(n^2))\n",
        "ce_score_margin = 3             # Margin for the CrossEncoder score between negative and positive passages\n",
        "num_negs_per_system = 5         # We used different systems to mine hard negatives. Number of hard negatives to add from each system\n",
        "\n",
        "##################################################\n",
        "#### Download MSMARCO Hard Negs Triplets File ####\n",
        "##################################################\n",
        "\n",
        "triplets_url = \"https://sbert.net/datasets/msmarco-hard-negatives.jsonl.gz\"\n",
        "msmarco_triplets_filepath = os.path.join(data_path, \"msmarco-hard-negatives.jsonl.gz\")\n",
        "if not os.path.isfile(msmarco_triplets_filepath):\n",
        "    util.download_url(triplets_url, msmarco_triplets_filepath)\n",
        "\n",
        "#### Load the hard negative MSMARCO jsonl triplets from SBERT \n",
        "#### These contain a ce-score which denotes the cross-encoder score for the query and passage.\n",
        "#### We chose a margin between positive and negative passage scores => above which consider negative as hard negative. \n",
        "#### Finally to limit the number of negatives per passage, we define num_negs_per_system across all different systems.\n",
        "\n",
        "logging.info(\"Loading MSMARCO hard-negatives...\")\n",
        "\n",
        "train_queries = {}\n",
        "with gzip.open(msmarco_triplets_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in tqdm(fIn, total=502939):\n",
        "        data = json.loads(line)\n",
        "        \n",
        "        #Get the positive passage ids\n",
        "        pos_pids = [item['pid'] for item in data['pos']]\n",
        "        pos_min_ce_score = min([item['ce-score'] for item in data['pos']])\n",
        "        ce_score_threshold = pos_min_ce_score - ce_score_margin\n",
        "        \n",
        "        #Get the hard negatives\n",
        "        neg_pids = set()\n",
        "        for system_negs in data['neg'].values():\n",
        "            negs_added = 0\n",
        "            for item in system_negs:\n",
        "                if item['ce-score'] > ce_score_threshold:\n",
        "                    continue\n",
        "\n",
        "                pid = item['pid']\n",
        "                if pid not in neg_pids:\n",
        "                    neg_pids.add(pid)\n",
        "                    negs_added += 1\n",
        "                    if negs_added >= num_negs_per_system:\n",
        "                        break\n",
        "        \n",
        "        if len(pos_pids) > 0 and len(neg_pids) > 0:\n",
        "            train_queries[data['qid']] = {'query': queries[data['qid']], 'pos': pos_pids, 'hard_neg': list(neg_pids)}\n",
        "        \n",
        "logging.info(\"Train queries: {}\".format(len(train_queries)))\n",
        "\n",
        "# We create a custom MSMARCO dataset that returns triplets (query, positive, negative)\n",
        "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
        "\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, queries, corpus):\n",
        "        self.queries = queries\n",
        "        self.queries_ids = list(queries.keys())\n",
        "        self.corpus = corpus\n",
        "\n",
        "        for qid in self.queries:\n",
        "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
        "            self.queries[qid]['hard_neg'] = list(self.queries[qid]['hard_neg'])\n",
        "            random.shuffle(self.queries[qid]['hard_neg'])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        query = self.queries[self.queries_ids[item]]\n",
        "        query_text = query['query']\n",
        "\n",
        "        pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
        "        pos_text = self.corpus[pos_id][\"text\"]\n",
        "        query['pos'].append(pos_id)\n",
        "\n",
        "        neg_id = query['hard_neg'].pop(0)    #Pop negative and add at end\n",
        "        neg_text = self.corpus[neg_id][\"text\"]\n",
        "        query['hard_neg'].append(neg_id)\n",
        "\n",
        "        return InputExample(texts=[query_text, pos_text, neg_text])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "# We construct the SentenceTransformer bi-encoder from scratch with mean-pooling\n",
        "model_name = \"distilbert-base-uncased\" \n",
        "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "#### Provide a high batch-size to train better with triplets!\n",
        "retriever = TrainRetriever(model=model, batch_size=train_batch_size)\n",
        "\n",
        "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
        "train_dataset = MSMARCODataset(train_queries, corpus=corpus)\n",
        "train_dataloader = retriever.prepare_train(train_dataset, shuffle=True, dataset_present=True)\n",
        "\n",
        "#### Training SBERT with cosine-product (default)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model)\n",
        "\n",
        "#### training SBERT with dot-product\n",
        "# train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model, similarity_fct=util.dot_score, scale=1)\n",
        "\n",
        "#### If no dev set is present from above use dummy evaluator\n",
        "ir_evaluator = retriever.load_dummy_evaluator()\n",
        "\n",
        "#### Provide model save path\n",
        "model_save_path = os.path.join(pathlib.Path(__file__).parent.absolute(), \"output\", \"{}-v3-{}\".format(model_name, dataset))\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "#### Configure Train params\n",
        "num_epochs = 10\n",
        "evaluation_steps = 10000\n",
        "warmup_steps = 1000\n",
        "\n",
        "retriever.fit(train_objectives=[(train_dataloader, train_loss)], \n",
        "                evaluator=ir_evaluator, \n",
        "                epochs=num_epochs,\n",
        "                output_path=model_save_path,\n",
        "                warmup_steps=warmup_steps,\n",
        "                evaluation_steps=evaluation_steps,\n",
        "                use_amp=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2561bbf4ec4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"msmarco\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_unzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    }
  ]
}